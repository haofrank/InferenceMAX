name: Claude Code

on:
  issue_comment:
    types: [created]
  issues:
    types: [opened, assigned]
  pull_request_review_comment:
    types: [created]

concurrency:
  group: claude-code-${{ github.event.issue.number }}
  cancel-in-progress: false

jobs:
  claude:
    if: |
      ((github.event_name == 'issue_comment' || github.event_name == 'pull_request_review_comment') && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'issues' && (contains(github.event.issue.body, '@claude') || contains(github.event.issue.title, '@claude')))
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      issues: write
      id-token: write
      actions: read
    steps:
      - name: Generate GitHub App token
        id: app-token
        uses: actions/create-github-app-token@v2
        with:
          app-id: ${{ secrets.APP_ID }}
          private-key: ${{ secrets.APP_PRIVATE_KEY }}

      - name: Checkout repository
        uses: actions/checkout@v6
        with:
          fetch-depth: 0
          token: ${{ steps.app-token.outputs.token }}

      - name: Setup MCP Server
        run: |
          pip3 install -r .claude/requirements-mcp.txt
          mkdir -p /tmp/inferencemax-mcp

      - name: Run Claude Code
        id: claude
        uses: anthropics/claude-code-action@v1
        env:
          GH_TOKEN: ${{ steps.app-token.outputs.token }}
          INFERENCEMAX_ROOT: ${{ github.workspace }}
          BASH_DEFAULT_TIMEOUT_MS: "1800000"
          BASH_MAX_TIMEOUT_MS: "3600000"
        with:
          anthropic_api_key: ${{ secrets.ANTHROPIC_API_KEY }}
          github_token: ${{ steps.app-token.outputs.token }}
          trigger_phrase: "@claude"
          track_progress: true
          allowed_bots: ''
          additional_permissions: |
            actions: read

          claude_args: |
            --model ${{ contains(github.event.comment.body || github.event.issue.body || '', '@claude sonnet') && 'claude-sonnet-4-5-20250929' || contains(github.event.comment.body || github.event.issue.body || '', '@claude haiku') && 'claude-haiku-4-5-20251001' || 'claude-opus-4-5-20251101' }}
            --mcp-config '{"mcpServers": {"fetch": {"command": "npx", "args": ["-y", "@anthropic-ai/mcp-server-fetch@latest"]}, "inferencemax-repos": {"command": "python3", "args": ["${{ github.workspace }}/.claude/mcp/server.py"], "env": {"INFERENCEMAX_ROOT": "${{ github.workspace }}"}}}}'
            --allowedTools "Write,Edit,Read,Glob,Grep,WebFetch,mcp__github__*,mcp__github_inline_comment__create_inline_comment,mcp__github_ci__*,mcp__fetch__*,mcp__inferencemax-repos__*,Bash"
          prompt: |
            REPO: ${{ github.repository }}
            PR/ISSUE NUMBER: ${{ github.event.pull_request.number || github.event.issue.number }}

            You are an AI assistant for InferenceMAX.

            **Workflow file modifications**: You CAN modify files in .github/workflows/ directory. The github_token parameter provides the necessary permissions for workflow file modifications.

            If you need to analyze benchmark results from a specific run, use:
            ```bash
            gh run download <RUN_ID> --repo ${{ github.repository }} -n results_bmk -D ./results
            cat ./results/agg_bmk.json | python3 -m json.tool
            ```
            
            To find recent benchmark runs:
            ```bash
            gh run list --repo ${{ github.repository }} --workflow e2e-tests.yml --limit 5
            ```
            
            You can analyze the json with:
            ```bash
            python3 <<'EOF'\nimport json \nwith open('agg_bmk.json') as f: data = json.load(f) \n# Your analysis code here \nEOF
            ```

            To trigger e2e tests, use the `mcp__github__run_workflow` tool to directly dispatch the e2e-tests.yml workflow.

            **Syntax:**
            ```
            mcp__github__run_workflow(
                owner="InferenceMAX",
                repo="InferenceMAX",
                workflow_id="e2e-tests.yml",
                ref="branch-name",
                inputs={
                    "generate-cli-command": "generator-cli-args",
                    "test-name": "Test description"
                }
            )
            ```

            The `generate-cli-command` input accepts arguments for `generate_sweep_configs.py`. Examples:

            **Filter by model prefix and Nvidia nodes:**
            ```
            generate-cli-command: "full-sweep --config-files .github/configs/nvidia-master.yaml --single-node --model-prefix dsr1"
            ```

            **Filter by framework and AMD nodes:**
            ```
            generate-cli-command: "full-sweep --config-files .github/configs/amd-master.yaml --single-node --framework sglang"
            ```

            **Filter by precision and runner type:**
            ```
            generate-cli-command: "full-sweep --config-files .github/configs/nvidia-master.yaml --single-node --precision fp8 --runner-type h200"
            ```

            **Test specific config keys:**
            ```
            generate-cli-command: "test-config --config-files .github/configs/nvidia-master.yaml --config-keys dsr1-fp4-b200-sglang"
            ```

            **IMPORTANT: Keep runs precise and efficient:**
            - You must use `--min-conc` and `--max-conc` together to specify a single concurrency value for targeted sweeps
            - You must use `--seq-len` to specify a single sequence length for targeted sweeps
            - Define specific config keys with `--config-keys` instead of running full sweeps
            - Filter by specific models, frameworks, or precision when possible
            - Never do a full sweep without filters unless explicitly instructed.

            ## Monitor workflow execution
            ```
            # Get workflow run details
            mcp__github__get_workflow_run(owner, repo, run_id)

            # List jobs for the run
            mcp__github__list_workflow_jobs(owner, repo, run_id)

            # Get logs for failed jobs
            mcp__github__get_job_logs(owner, repo, run_id=run_id, failed_only=true)
            ```

            **When to trigger e2e tests:**
            - When directly asked to run performance tests
            - When performance testing is needed
            - After reviewing code changes that might affect performance
            - For all runs, ensure they have links in the comment.

            After triggering, monitor the workflow run using the returned run_id. Wait until it completes before analyzing results.
            - Do NOT claim completion until most recent job finishes and results analyzed.
            - You can do a long `sleep` command to wait for the job to finish.
            - However, you can analyze an ongoing run, for example if it errors, and start a new run in parallel without finishing the old run. Cancel such old runs.
            - If jobs cannot be run, say exactly what you could not run and why.

            ## vLLM and SGLang Source Code Access

            You have access to vLLM and SGLang source code via the inferencemax-repos MCP server:
            - Use `mcp__inferencemax_repos__*` tools to access repository source code
            - Resources are available via URIs: `vllm:///path/to/file.py` and `sglang:///path/to/file.py`
            - The server automatically detects and checks out the version matching InferenceMAX configs
            - Use the `list_versions` tool to see detected versions
            - Use the `switch_version` tool to switch to a different version if needed

            This gives you deep context about vLLM and SGLang internals when debugging issues or explaining behavior.

            Focus on: code quality, benchmark config changes, and performance impact. Do not be lazy.

            ## Web Access:
            You have internet access via MCP servers:
            - `mcp__fetch__fetch` - Fetch content from any URL

            ### Useful Documentation URLs:
            - **sglang**: https://docs.sglang.ai/
            - **vLLM**: https://docs.vllm.ai/en/latest/
