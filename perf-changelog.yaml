- config-keys:
    - 70b-fp8-*-vllm
  description:
    - 'Add compilation-config ''{"custom_ops": ["-rms_norm", "-quant_fp8", "-silu_and_mul"]}'' as extra config to all benchmarks/70b_fp8_mi*.sh scripts'
    - "6-7% uplift for llama for 6/8 configs"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/95

- config-keys:
    - gptoss-fp4-*-trt
  description:
    - "Upgrade GPT-OSS TRT images from 'release:1.1.0rc2.post2' to '1.2.0rc0.post1'"
    - "Add NCCL_GRAPH_REGISTER=0 to benchmarks/gptoss_fp4_b200_trt_slurm.sh"
    - "Change kv_cache_config.dtype from 'auto' to 'fp8' in benchmarks/gptoss_fp4_b200_trt_slurm.sh"
    - "Remove MOE_BACKEND=CUTLASS, now just defaults to TRTLLM"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/110

- config-keys:
    - gptoss*
    - dsr1*
  description:
    - "Remove Llama 70B runs to make room for multi-node disagg prefill+wideEP on h100/h200/b200/mi300/mi325/mi355"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/149

- config-keys:
    - gptoss-fp4-b200-vllm
    - gptoss-fp4-h100-vllm
    - gptoss-fp4-h200-vllm
  description:
    - "Upgrade vLLM from 0.10.2 to 0.11.0 for GPT-OSS NVIDIA single-node configs"
    - 'Add compilation-config ''{"cudagraph_mode":"PIECEWISE"}'' since vLLM 0.11.0 now defaults to FULL_AND_PIECEWISE'
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/159

- config-keys:
    - dsr1*
  description:
    - "Fix bug where 1k8k and 8k1k full sweeps had incorrect max-model-len for DeepSeek"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/163

- config-keys:
    - dsr1-fp4-b200-sglang
    - dsr1-fp8-b200-sglang
    - dsr1-fp8-h200-sglang
  description:
    - "Consolidate H200 and B200 SGLang configurations to use unified v0.5.5-cu129-amd64 image tag"
    - "Update deprecated SGLang server arguments to current equivalents"
    - "Replace --enable-ep-moe with --ep-size $EP_SIZE"
    - "Replace --enable-flashinfer-trtllm-moe with --moe-runner-backend flashinfer_trtllm"
    - "Add -e EP_SIZE to Docker run commands in launch scripts"
    - "Set ep:4 for all tp:4 entries, ep:8 for all tp:8 entries"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/204

- config-keys:
    - gptoss-fp4-mi355x-vllm
    - gptoss-fp4-b200-vllm
  description:
    - "Extend concurrency to 128 for gptoss mi355x/b200 vllm configurations"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/209

- config-keys:
    - gptoss-fp4-b200-trt
  description:
    - "Extend concurrency to 128 for gptoss b200 TRT configurations"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/233

- config-keys:
    - "*gb200-dynamo-sglang"
  description:
    - "Introduce improvements in GB200 SGLang DSR1 submission"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/257

- config-keys:
    - dsr1-fp8-h200-trt
  description:
    - "Update TRT image from nvcr.io#nvidia/tensorrt-llm/release:1.2.0rc0.post1 to nvcr.io#nvidia/tensorrt-llm/release:1.2.0rc2"
    - "Increase concurrency for some configurations"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/266

- config-keys:
    - gptoss-fp4-b200-vllm
    - gptoss-fp4-h100-vllm
    - gptoss-fp4-h200-vllm
  description:
    - "Update vLLM image for NVIDIA configs from vLLM 0.11.0 to vLLM 0.11.2"
    - "Add kv-cache-dtype: fp8 to benchmarks/gptoss_fp4_b200_docker.sh"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/273

- config-keys:
    - gptoss-fp4-b200-trt
  description:
    - "Add benchmark script for GPTOSS FP4 B200 TRT-LLM"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/256

- config-keys:
    - dsr1-fp4-gb200-dynamo-trt
    - dsr1-fp4-gb200-dynamo-sglang
    - dsr1-fp8-gb200-dynamo-sglang
  description:
    - "Add more configurations for GB200 SGLang DSR1"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/335

- config-keys:
    - dsr1-fp4-mi355x-sglang
  description:
    - "Update MI355x Deepseek-R1 FP4 SGLang Image to upstream v0.5.6.post1"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/330

- config-keys:
    - dsr1-fp4-gb200-dynamo-sglang
    - dsr1-fp8-gb200-dynamo-sglang
  description:
    - "fix: Pruning unnecessary concurrencies "
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/358

- config-keys:
    - dsr1-fp4-mi355x-sglang
  description:
    - "Updating MI355x Deepseek-R1 FP4 SGLang Image to upstream v0.5.6.post2"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/369
    
- config-keys:
    - dsr1-fp4-b200-sglang
    - dsr1-fp8-b200-sglang
    - dsr1-fp8-h200-sglang
  description:
    - "Update NVIDIA DeepSeek sglang Docker image from v0.5.5 to v0.5.6"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/276
  
- config-keys:
    - gptoss-fp4-b200-vllm
    - gptoss-fp4-h100-vllm
    - gptoss-fp4-h200-vllm
  description: 
    - "Update vLLM image from v0.11.2 to v0.13.0"
    - "Add VLLM_MXFP4_USE_MARLIN=1 to H100 and H200 benchmark scripts"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/327

- config-keys:
    - dsr1-fp8-mi300x-sglang
    - dsr1-fp8-mi325x-sglang
    - dsr1-fp8-mi355x-sglang
  description:
    - Use upstream SGLang images on mi300, mi325 and mi355 for dsr1fp8
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/332

- config-keys:
    - gptoss-fp4-gb200-dynamo-trt
    - gptoss-fp4-b200-trt
  description:
    - Explicitly add EP=TP for DP attention configs for B200 AGG nvidia-master file. Multinode Refactor inadvertently changed default EP=1
    - Add GPTOSS DISAGG configurations for GB200 1k1k and 8k1k.
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/387

- config-keys:
    - dsr1-fp8-mi355x-sglang-disagg
  description:
    - "Add PD disaggregation (1P2D) for Mi355X"
    - "Includes with and without speculative decoding"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/348

- config-keys:
    - dsr1-fp4-mi355x-sglang
  description:
    - "Updating MI355x Deepseek-R1 FP4 SGLang Image to upstream v0.5.7"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/395

- config-keys:
    - dsr1-fp8-b200-sglang
  description:
    - "Adds TP4 configurations to DSR1-FP8 B200 SGLang deployment experiments"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/411
  
- config-keys:
    - dsr1-fp4-b200-trt-mtp
    - dsr1-fp8-b200-trt-mtp
    - dsr1-fp8-h200-trt-mtp
  description:
    - Add MTP (Multi-Token Prediction) support for single-node TRT configs
    - Add spec-decoding field to config entries and update launch scripts to select MTP benchmark scripts
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/392
  
- config-keys:
    - dsr1-fp8-mi355x-atom
    - dsr1-fp4-mi355x-atom
    - gptoss-fp4-mi355x-atom
  description:
    - Add internal AMD ATOM inference engine for DeepSeek R1 FP8, FP4 and GPTOSS FP4 Mi355X
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/419

- config-keys:
    - gptoss-fp4-mi300x-vllm
    - gptoss-fp4-mi325x-vllm
  description:
    - "Update AMD MI300X and MI325X GPT-OSS 120B vLLM to use upstream ROCm image vllm/vllm-openai-rocm:v0.14.0"
    - "Remove deprecated --async-scheduling flag (now enabled by default in vLLM v0.14.0)"
    - "Remove deprecated --max-seq-len-to-capture flag"
    - "Add HIP_VISIBLE_DEVICES env var for Ray compatibility in vLLM 0.14+"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/496

- config-keys:
    - dsr1-fp8-h200-sglang
  description:
    - "Update H200 DeepSeek R1 FP8 SGLang image from v0.5.6 to v0.5.7"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/538

- config-keys:
    - dsr1-fp8-mi300x-sglang
  description:
    - "Update MI300X DeepSeek R1 FP8 SGLang image from v0.5.5.post3 to v0.5.7"
    - "Add SGLANG_AITER_MLA_PERSIST=1 for persistent MLA kernel optimization"
    - "Set --kv-cache-dtype fp8_e4m3 for fp8 KV cache"
    - "Set --attention-backend aiter for AMD aiter attention backend"
    - "Update chunked-prefill-size and max-prefill-tokens from 196608 to 131072"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/544
- config-keys:
    - dsr1-fp8-mi325x-sglang
  description:
    - "Update MI325X DeepSeek R1 FP8 SGLang image from v0.5.5.post3 to v0.5.7"
    - "Add SGLANG_AITER_MLA_PERSIST=1 for persistent MLA kernel with fp8 KV cache"
    - "Add --kv-cache-dtype fp8_e4m3 for explicit FP8 KV cache"
    - "Add --attention-backend aiter for AMD aiter attention backend"
    - "Reduce chunked-prefill-size from 196608 to 131072"
    - "Reduce max-prefill-tokens from 196608 to 131072"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/545

- config-keys:
    - gptoss-fp4-mi300x-vllm
    - gptoss-fp4-mi325x-vllm
  description:
    - "Fix AITER env vars for vLLM v0.14.0 on AMD MI300X and MI325X"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/535

- config-keys:
    - dsr1-fp8-mi355x-sglang
  description:
    - "Update MI355X DeepSeek R1 FP8 SGLang image from v0.5.5.post3 to v0.5.8"
    - "Key fix: Disables mla persistent kernel when not using fp8 kv_cache (https://github.com/sgl-project/sglang/pull/17327)"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/572

- config-keys:
    # NVIDIA single-node
    - dsr1-fp4-b200-sglang
    - dsr1-fp4-b200-trt
    - dsr1-fp4-b200-trt-mtp
    - dsr1-fp8-b200-sglang
    - dsr1-fp8-b200-trt
    - dsr1-fp8-b200-trt-mtp
    - dsr1-fp8-h200-sglang
    - dsr1-fp8-h200-trt
    - dsr1-fp8-h200-trt-mtp
    - gptoss-fp4-b200-trt
    - gptoss-fp4-b200-vllm
    - gptoss-fp4-h100-vllm
    - gptoss-fp4-h200-trt
    - gptoss-fp4-h200-vllm
    # AMD single-node
    - dsr1-fp4-mi355x-sglang
    - dsr1-fp4-mi355x-atom
    - dsr1-fp8-mi300x-sglang
    - dsr1-fp8-mi325x-sglang
    - dsr1-fp8-mi355x-sglang
    - dsr1-fp8-mi355x-atom
    - gptoss-fp4-mi300x-vllm
    - gptoss-fp4-mi325x-vllm
    - gptoss-fp4-mi355x-vllm
    - gptoss-fp4-mi355x-atom
  description:
    - Add official GSM8k eval results to GPT-OSS and DeepSeek R1 scenarios
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/558
  evals-only: true
  
- config-keys:
  - dsr1-fp4-b300-dynamo-trt
  description:
    - "Add DSR1 FP4 B300 Dynamo TRT configurations"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/585

- config-keys:
  - dsr1-fp4-mi355x-sglang
  description:
    - "Update SGLang image from v0.5.7 to v0.5.8 for DeepSeek-R1 FP4 on MI355x"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/595
  
- config-keys:
    - dsr1-fp8-b200-trt
  description:
    - "Update TensorRT-LLM container from release:1.1.0rc2.post2 to release:1.2.0rc6.post2"
    - "Change default MOE backend from DEEPGEMM to TRTLLM"
    - "Add dynamic piecewise CUDA graphs for 1k1k (CONC≥64) and 8k1k (CONC≥64) workloads"
    - "Add delay batching (batch_wait_timeout_iters/batch_wait_max_tokens_ratio) for 1k1k high-concurrency"
    - "Add dynamic KV cache memory fraction tuning (0.7-0.8) based on ISL/OSL/TP configuration"
    - "Update search space: remove EP=TP constraint, add TP=4 configurations, extend concurrency ranges"
    - "Add TLLM_OVERRIDE_LAYER_NUM=61 to avoid OOM errors"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/594
  
- config-keys:
  - dsr1-fp4-b200-dynamo-trt
  description:
    - "Update DSR1 FP4 B200 Dynamo TRT configurations"
    - "Update TRTLLM version to 1.2.0rc6.post2"
    - "Transform to use srt-slurm recipes"
  pr-link: https://github.com/InferenceMAX/InferenceMAX/pull/588
